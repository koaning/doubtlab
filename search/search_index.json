{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"doubtlab DoubtLab helps you find bad labels. This repository contains general tricks that may help you find bad, or noisy, labels in your dataset. The hope is that this repository makes it easier for folks to quickly check their own datasets before they invest too much time and compute on gridsearch. Installation \u00b6 You can install the tool via pip or conda . Install with pip python -m pip install doubtlab Install with conda conda install -c conda-forge doubtlab Getting Started \u00b6 If you want to get started, we recommend starting here . Related Projects \u00b6 The cleanlab project was an inspiration for this one. They have a great heuristic for bad label detection but I wanted to have a library that implements many. Be sure to check out their work on the labelerrors.com project. My former employer, Rasa , has always had a focus on data quality. Some of that attitude is bound to have seeped in here. Be sure to check the Conversation Driven Development approach and Rasa X if you're working on virtual assistants. My current employer, Explosion , has a neat labelling tool called prodigy . I'm currently investigating how tools like doubtlab might lead to better labels when combined with this (very like-able) annotation tool.","title":"Home"},{"location":"#installation","text":"You can install the tool via pip or conda . Install with pip python -m pip install doubtlab Install with conda conda install -c conda-forge doubtlab","title":"Installation"},{"location":"#getting-started","text":"If you want to get started, we recommend starting here .","title":"Getting Started"},{"location":"#related-projects","text":"The cleanlab project was an inspiration for this one. They have a great heuristic for bad label detection but I wanted to have a library that implements many. Be sure to check out their work on the labelerrors.com project. My former employer, Rasa , has always had a focus on data quality. Some of that attitude is bound to have seeped in here. Be sure to check the Conversation Driven Development approach and Rasa X if you're working on virtual assistants. My current employer, Explosion , has a neat labelling tool called prodigy . I'm currently investigating how tools like doubtlab might lead to better labels when combined with this (very like-able) annotation tool.","title":"Related Projects"},{"location":"api/benchmark/","text":"calculate_precision_recall_at_k ( predicate_df , idx_flip , max_k = 100 , give_random = False , give_ensemble = True ) \u00b6 Show source code in doubtlab/benchmark.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def calculate_precision_recall_at_k ( predicate_df , idx_flip , max_k = 100 , give_random = False , give_ensemble = True ): \"\"\" Plots precision/recall at `k` values for flipped label experiments. Returns an interactive altair visualisation. Make sure it is installed beforehand. Arguments: predicate_df: the dataframe with predicates from `ensemble.get_predicates` idx_flip: array that indicates if labels are wrong max_k: the maximum value for `k` to consider give_random: plot the \"at k\" statistics for the randomly selected lower bound give_ensemble: plot the \"at k\" statistics from the reason ensemble \"\"\" # First we need to ensure that the original dataframe with X values is # is combined with our reasons dataframe and sorted appropriately. df = predicate_df . assign ( s = lambda d : d [[ c for c in d . columns if \"predicate\" in c ]] . sum ( axis = 1 ), flipped = idx_flip , ) . sort_values ( \"s\" , ascending = False ) # Next we calculate the precision/recall at k values data = [] for k in range ( 1 , max_k ): recall_at_k = df [ \"flipped\" ][: k ] . sum () / df [ \"flipped\" ] . sum () precision_at_k = ( df [ \"flipped\" ][: k ] == np . ones ( k )) . sum () / k random_recall = df [ \"flipped\" ] . mean () * k / df [ \"flipped\" ] . sum () random_precision = df [ \"flipped\" ] . mean () data . append ( { \"recall_at_k\" : recall_at_k , \"precision_at_k\" : precision_at_k , \"k\" : k , \"setting\" : \"ensemble\" , } ) data . append ( { \"recall_at_k\" : random_recall , \"precision_at_k\" : random_precision , \"k\" : k , \"setting\" : \"random\" , } ) result = pd . DataFrame ( data ) . melt ([ \"k\" , \"setting\" ]) # Give the user the option to only return draw a subset if not give_random : result = result . loc [ lambda d : d [ \"setting\" ] != \"random\" ] if not give_ensemble : result = result . loc [ lambda d : d [ \"setting\" ] != \"ensemble\" ] # Return the data in a tidy format. return result Plots precision/recall at k values for flipped label experiments. Returns an interactive altair visualisation. Make sure it is installed beforehand. Parameters Name Type Description Default predicate_df the dataframe with predicates from ensemble.get_predicates required idx_flip array that indicates if labels are wrong required max_k the maximum value for k to consider 100 give_random plot the \"at k\" statistics for the randomly selected lower bound False give_ensemble plot the \"at k\" statistics from the reason ensemble True flip_labels ( y , random_seed = 42 , n = None , p = None ) \u00b6 Show source code in doubtlab/benchmark.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def flip_labels ( y , random_seed = 42 , n = None , p = None ): \"\"\" Flips subset of labels for benchmarking. Recommended for classification. Either `p` or `n` should be given. Returns a tuple `(y_out, indicator)`-tuple. Arguments: y: array of labels random_seed: random seed n: number of labels to flip p: percentage of labels to flip Usage: ```python import numpy as np from doubtlab.benchmark import flip_labels # Let's pretend these are the actual labels y = np.random.randint(0, 3, 10000) # You now have some shuffled labels and an indicator y_out, indicator = flip_labels(y, n=100) ``` \"\"\" np . random . seed ( random_seed ) y = np . array ( y ) n = _parse_check_p_n_y ( p = p , n = n , y = y ) y_out = y . copy () classes = np . unique ( y ) if len ( classes ) == 1 : raise ValueError ( \"Need more that 1 class in `y`.\" ) # Only sample classes that didn't appear before. idx = np . random . choice ( np . arange ( y . shape [ 0 ]), size = n , replace = False ) y_out [ idx ] = [ np . random . choice ( classes [ classes != _ ]) for _ in y_out [ idx ]] return y_out , ( y != y_out ) . astype ( int ) Flips subset of labels for benchmarking. Recommended for classification. Either p or n should be given. Returns a tuple (y_out, indicator) -tuple. Parameters Name Type Description Default y array of labels required random_seed random seed 42 n number of labels to flip None p percentage of labels to flip None Usage: import numpy as np from doubtlab.benchmark import flip_labels # Let's pretend these are the actual labels y = np . random . randint ( 0 , 3 , 10000 ) # You now have some shuffled labels and an indicator y_out , indicator = flip_labels ( y , n = 100 ) plot_precision_recall_at_k ( predicate_df , idx_flip , max_k = 100 , give_random = True , give_ensemble = True ) \u00b6 Show source code in doubtlab/benchmark.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def plot_precision_recall_at_k ( predicate_df , idx_flip , max_k = 100 , give_random = True , give_ensemble = True ): \"\"\" Plots precision/recall at `k` values for flipped label experiments. Returns an interactive altair visualisation. Make sure it is installed beforehand. Arguments: predicate_df: the dataframe with predicates from `ensemble.get_predicates` idx_flip: array that indicates if labels are wrong max_k: the maximum value for `k` to consider give_random: plot the \"at k\" statistics for the randomly selected lower bound give_ensemble: plot the \"at k\" statistics from the reason ensemble \"\"\" import altair as alt alt . data_transformers . disable_max_rows () # We combine the results in dataframes plot_df = calculate_precision_recall_at_k ( predicate_df = predicate_df , idx_flip = idx_flip , max_k = max_k , give_random = give_random , give_ensemble = give_ensemble , ) # So that we may plot it. return ( alt . Chart ( plot_df ) . mark_line () . encode ( x = \"k\" , y = \"value\" , color = \"variable\" , strokeDash = \"setting\" ) . interactive () ) Plots precision/recall at k values for flipped label experiments. Returns an interactive altair visualisation. Make sure it is installed beforehand. Parameters Name Type Description Default predicate_df the dataframe with predicates from ensemble.get_predicates required idx_flip array that indicates if labels are wrong required max_k the maximum value for k to consider 100 give_random plot the \"at k\" statistics for the randomly selected lower bound True give_ensemble plot the \"at k\" statistics from the reason ensemble True shuffle_labels ( y , random_seed = 42 , n = None , p = None ) \u00b6 Show source code in doubtlab/benchmark.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def shuffle_labels ( y , random_seed = 42 , n = None , p = None ): \"\"\" Shuffles subset of labels for benchmarking. Recommended for regression. Either `p` or `n` should be given. Returns a tuple `(y_out, indicator)`-tuple. Arguments: y: array of labels random_seed: random seed n: number of labels to flip p: percentage of labels to flip Usage: ```python import numpy as np from doubtlab.benchmark import shuffle_labels # Let's pretend these are the actual labels y = np.random.normal(0, 1, 10000) # You now have some shuffled labels and an indicator y_out, indicator = shuffle_labels(y, n=100) ``` \"\"\" np . random . seed ( random_seed ) y = np . array ( y ) n = _parse_check_p_n_y ( p = p , n = n , y = y ) y_out = y . copy () sample = np . random . choice ( np . arange ( y . shape [ 0 ]), size = n , replace = False ) # Since `sample` is already randomly shuffled, we can move everything # over by one index to guarantee a shuffle of the values from another index. y_out [ sample ] = np . concatenate ([ sample [ 1 :], sample [ 0 : 1 ]]) return y_out , ( y != y_out ) . astype ( int ) Shuffles subset of labels for benchmarking. Recommended for regression. Either p or n should be given. Returns a tuple (y_out, indicator) -tuple. Parameters Name Type Description Default y array of labels required random_seed random seed 42 n number of labels to flip None p percentage of labels to flip None Usage: import numpy as np from doubtlab.benchmark import shuffle_labels # Let's pretend these are the actual labels y = np . random . normal ( 0 , 1 , 10000 ) # You now have some shuffled labels and an indicator y_out , indicator = shuffle_labels ( y , n = 100 )","title":"benchmark"},{"location":"api/benchmark/#doubtlab.benchmark.calculate_precision_recall_at_k","text":"Show source code in doubtlab/benchmark.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def calculate_precision_recall_at_k ( predicate_df , idx_flip , max_k = 100 , give_random = False , give_ensemble = True ): \"\"\" Plots precision/recall at `k` values for flipped label experiments. Returns an interactive altair visualisation. Make sure it is installed beforehand. Arguments: predicate_df: the dataframe with predicates from `ensemble.get_predicates` idx_flip: array that indicates if labels are wrong max_k: the maximum value for `k` to consider give_random: plot the \"at k\" statistics for the randomly selected lower bound give_ensemble: plot the \"at k\" statistics from the reason ensemble \"\"\" # First we need to ensure that the original dataframe with X values is # is combined with our reasons dataframe and sorted appropriately. df = predicate_df . assign ( s = lambda d : d [[ c for c in d . columns if \"predicate\" in c ]] . sum ( axis = 1 ), flipped = idx_flip , ) . sort_values ( \"s\" , ascending = False ) # Next we calculate the precision/recall at k values data = [] for k in range ( 1 , max_k ): recall_at_k = df [ \"flipped\" ][: k ] . sum () / df [ \"flipped\" ] . sum () precision_at_k = ( df [ \"flipped\" ][: k ] == np . ones ( k )) . sum () / k random_recall = df [ \"flipped\" ] . mean () * k / df [ \"flipped\" ] . sum () random_precision = df [ \"flipped\" ] . mean () data . append ( { \"recall_at_k\" : recall_at_k , \"precision_at_k\" : precision_at_k , \"k\" : k , \"setting\" : \"ensemble\" , } ) data . append ( { \"recall_at_k\" : random_recall , \"precision_at_k\" : random_precision , \"k\" : k , \"setting\" : \"random\" , } ) result = pd . DataFrame ( data ) . melt ([ \"k\" , \"setting\" ]) # Give the user the option to only return draw a subset if not give_random : result = result . loc [ lambda d : d [ \"setting\" ] != \"random\" ] if not give_ensemble : result = result . loc [ lambda d : d [ \"setting\" ] != \"ensemble\" ] # Return the data in a tidy format. return result Plots precision/recall at k values for flipped label experiments. Returns an interactive altair visualisation. Make sure it is installed beforehand. Parameters Name Type Description Default predicate_df the dataframe with predicates from ensemble.get_predicates required idx_flip array that indicates if labels are wrong required max_k the maximum value for k to consider 100 give_random plot the \"at k\" statistics for the randomly selected lower bound False give_ensemble plot the \"at k\" statistics from the reason ensemble True","title":"calculate_precision_recall_at_k()"},{"location":"api/benchmark/#doubtlab.benchmark.flip_labels","text":"Show source code in doubtlab/benchmark.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def flip_labels ( y , random_seed = 42 , n = None , p = None ): \"\"\" Flips subset of labels for benchmarking. Recommended for classification. Either `p` or `n` should be given. Returns a tuple `(y_out, indicator)`-tuple. Arguments: y: array of labels random_seed: random seed n: number of labels to flip p: percentage of labels to flip Usage: ```python import numpy as np from doubtlab.benchmark import flip_labels # Let's pretend these are the actual labels y = np.random.randint(0, 3, 10000) # You now have some shuffled labels and an indicator y_out, indicator = flip_labels(y, n=100) ``` \"\"\" np . random . seed ( random_seed ) y = np . array ( y ) n = _parse_check_p_n_y ( p = p , n = n , y = y ) y_out = y . copy () classes = np . unique ( y ) if len ( classes ) == 1 : raise ValueError ( \"Need more that 1 class in `y`.\" ) # Only sample classes that didn't appear before. idx = np . random . choice ( np . arange ( y . shape [ 0 ]), size = n , replace = False ) y_out [ idx ] = [ np . random . choice ( classes [ classes != _ ]) for _ in y_out [ idx ]] return y_out , ( y != y_out ) . astype ( int ) Flips subset of labels for benchmarking. Recommended for classification. Either p or n should be given. Returns a tuple (y_out, indicator) -tuple. Parameters Name Type Description Default y array of labels required random_seed random seed 42 n number of labels to flip None p percentage of labels to flip None Usage: import numpy as np from doubtlab.benchmark import flip_labels # Let's pretend these are the actual labels y = np . random . randint ( 0 , 3 , 10000 ) # You now have some shuffled labels and an indicator y_out , indicator = flip_labels ( y , n = 100 )","title":"flip_labels()"},{"location":"api/benchmark/#doubtlab.benchmark.plot_precision_recall_at_k","text":"Show source code in doubtlab/benchmark.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def plot_precision_recall_at_k ( predicate_df , idx_flip , max_k = 100 , give_random = True , give_ensemble = True ): \"\"\" Plots precision/recall at `k` values for flipped label experiments. Returns an interactive altair visualisation. Make sure it is installed beforehand. Arguments: predicate_df: the dataframe with predicates from `ensemble.get_predicates` idx_flip: array that indicates if labels are wrong max_k: the maximum value for `k` to consider give_random: plot the \"at k\" statistics for the randomly selected lower bound give_ensemble: plot the \"at k\" statistics from the reason ensemble \"\"\" import altair as alt alt . data_transformers . disable_max_rows () # We combine the results in dataframes plot_df = calculate_precision_recall_at_k ( predicate_df = predicate_df , idx_flip = idx_flip , max_k = max_k , give_random = give_random , give_ensemble = give_ensemble , ) # So that we may plot it. return ( alt . Chart ( plot_df ) . mark_line () . encode ( x = \"k\" , y = \"value\" , color = \"variable\" , strokeDash = \"setting\" ) . interactive () ) Plots precision/recall at k values for flipped label experiments. Returns an interactive altair visualisation. Make sure it is installed beforehand. Parameters Name Type Description Default predicate_df the dataframe with predicates from ensemble.get_predicates required idx_flip array that indicates if labels are wrong required max_k the maximum value for k to consider 100 give_random plot the \"at k\" statistics for the randomly selected lower bound True give_ensemble plot the \"at k\" statistics from the reason ensemble True","title":"plot_precision_recall_at_k()"},{"location":"api/benchmark/#doubtlab.benchmark.shuffle_labels","text":"Show source code in doubtlab/benchmark.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def shuffle_labels ( y , random_seed = 42 , n = None , p = None ): \"\"\" Shuffles subset of labels for benchmarking. Recommended for regression. Either `p` or `n` should be given. Returns a tuple `(y_out, indicator)`-tuple. Arguments: y: array of labels random_seed: random seed n: number of labels to flip p: percentage of labels to flip Usage: ```python import numpy as np from doubtlab.benchmark import shuffle_labels # Let's pretend these are the actual labels y = np.random.normal(0, 1, 10000) # You now have some shuffled labels and an indicator y_out, indicator = shuffle_labels(y, n=100) ``` \"\"\" np . random . seed ( random_seed ) y = np . array ( y ) n = _parse_check_p_n_y ( p = p , n = n , y = y ) y_out = y . copy () sample = np . random . choice ( np . arange ( y . shape [ 0 ]), size = n , replace = False ) # Since `sample` is already randomly shuffled, we can move everything # over by one index to guarantee a shuffle of the values from another index. y_out [ sample ] = np . concatenate ([ sample [ 1 :], sample [ 0 : 1 ]]) return y_out , ( y != y_out ) . astype ( int ) Shuffles subset of labels for benchmarking. Recommended for regression. Either p or n should be given. Returns a tuple (y_out, indicator) -tuple. Parameters Name Type Description Default y array of labels required random_seed random seed 42 n number of labels to flip None p percentage of labels to flip None Usage: import numpy as np from doubtlab.benchmark import shuffle_labels # Let's pretend these are the actual labels y = np . random . normal ( 0 , 1 , 10000 ) # You now have some shuffled labels and an indicator y_out , indicator = shuffle_labels ( y , n = 100 )","title":"shuffle_labels()"},{"location":"api/doubtlab/","text":"DoubtEnsemble \u00b6 A pipeline to find bad labels. Parameters Name Type Description Default **reasons kwargs with (name, reason)-pairs {} Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons ) get_indices ( self , X , y = None ) \u00b6 Show source code in doubtlab/ensemble.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def get_indices ( self , X , y = None ): \"\"\" Calculates indices worth checking again. Arguments: X: the `X` data to be processed y: the `y` data to be processed Usage: ```python from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason, WrongPredictionReason X, y = load_iris(return_X_y=True) model = LogisticRegression(max_iter=1_000) model.fit(X, y) reasons = { \"proba\": ProbaReason(model=model), \"wrong_pred\": WrongPredictionReason(model=model), } doubt = DoubtEnsemble(**reasons) indices = doubt.get_indices(X, y) ``` \"\"\" df = self . get_predicates ( X , y ) predicates = [ c for c in df . columns if isinstance ( c , str ) and ( \"predicate\" in c ) ] df = ( df [ predicates ] . assign ( s = lambda d : d [ predicates ] . sum ( axis = 1 )) . sort_values ([ \"s\" ], ascending = False ) . loc [ lambda d : d [ \"s\" ] > 0 ] ) return np . array ( df . index ) Calculates indices worth checking again. Parameters Name Type Description Default X the X data to be processed required y the y data to be processed None Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons ) indices = doubt . get_indices ( X , y ) get_predicates ( self , X , y = None ) \u00b6 Show source code in doubtlab/ensemble.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def get_predicates ( self , X , y = None ): \"\"\" Returns a sorted dataframe that shows the reasoning behind the sorting. Arguments: X: the `X` data to be processed y: the `y` data to be processed Usage: ```python from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason, WrongPredictionReason X, y = load_iris(return_X_y=True) model = LogisticRegression(max_iter=1_000) model.fit(X, y) reasons = { \"proba\": ProbaReason(model=model), \"wrong_pred\": WrongPredictionReason(model=model), } doubt = DoubtEnsemble(**reasons) predicates = doubt.get_predicates(X, y) ``` \"\"\" df = pd . DataFrame ( { f \"predicate_ { name } \" : func ( X , y ) for name , func in self . reasons . items ()} ) sorted_index = df . sum ( axis = 1 ) . sort_values ( ascending = False ) . index return df . reindex ( sorted_index ) Returns a sorted dataframe that shows the reasoning behind the sorting. Parameters Name Type Description Default X the X data to be processed required y the y data to be processed None Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons ) predicates = doubt . get_predicates ( X , y )","title":"ensemble"},{"location":"api/doubtlab/#doubtensemble","text":"A pipeline to find bad labels. Parameters Name Type Description Default **reasons kwargs with (name, reason)-pairs {} Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons )","title":"DoubtEnsemble"},{"location":"api/doubtlab/#doubtlab.ensemble.DoubtEnsemble.get_indices","text":"Show source code in doubtlab/ensemble.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def get_indices ( self , X , y = None ): \"\"\" Calculates indices worth checking again. Arguments: X: the `X` data to be processed y: the `y` data to be processed Usage: ```python from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason, WrongPredictionReason X, y = load_iris(return_X_y=True) model = LogisticRegression(max_iter=1_000) model.fit(X, y) reasons = { \"proba\": ProbaReason(model=model), \"wrong_pred\": WrongPredictionReason(model=model), } doubt = DoubtEnsemble(**reasons) indices = doubt.get_indices(X, y) ``` \"\"\" df = self . get_predicates ( X , y ) predicates = [ c for c in df . columns if isinstance ( c , str ) and ( \"predicate\" in c ) ] df = ( df [ predicates ] . assign ( s = lambda d : d [ predicates ] . sum ( axis = 1 )) . sort_values ([ \"s\" ], ascending = False ) . loc [ lambda d : d [ \"s\" ] > 0 ] ) return np . array ( df . index ) Calculates indices worth checking again. Parameters Name Type Description Default X the X data to be processed required y the y data to be processed None Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons ) indices = doubt . get_indices ( X , y )","title":"get_indices()"},{"location":"api/doubtlab/#doubtlab.ensemble.DoubtEnsemble.get_predicates","text":"Show source code in doubtlab/ensemble.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def get_predicates ( self , X , y = None ): \"\"\" Returns a sorted dataframe that shows the reasoning behind the sorting. Arguments: X: the `X` data to be processed y: the `y` data to be processed Usage: ```python from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason, WrongPredictionReason X, y = load_iris(return_X_y=True) model = LogisticRegression(max_iter=1_000) model.fit(X, y) reasons = { \"proba\": ProbaReason(model=model), \"wrong_pred\": WrongPredictionReason(model=model), } doubt = DoubtEnsemble(**reasons) predicates = doubt.get_predicates(X, y) ``` \"\"\" df = pd . DataFrame ( { f \"predicate_ { name } \" : func ( X , y ) for name , func in self . reasons . items ()} ) sorted_index = df . sum ( axis = 1 ) . sort_values ( ascending = False ) . index return df . reindex ( sorted_index ) Returns a sorted dataframe that shows the reasoning behind the sorting. Parameters Name Type Description Default X the X data to be processed required y the y data to be processed None Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) reasons = { \"proba\" : ProbaReason ( model = model ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } doubt = DoubtEnsemble ( ** reasons ) predicates = doubt . get_predicates ( X , y )","title":"get_predicates()"},{"location":"api/reasons/","text":"from doubtlab.reason import * \u00b6 AbsoluteDifferenceReason \u00b6 Assign doubt when the absolute difference between label and regression is too large. Parameters Name Type Description Default model scikit-learn regression model required threshold cutoff for doubt assignment required Usage: from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import AbsoluteDifferenceReason X , y = load_diabetes ( return_X_y = True ) model = LinearRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = AbsoluteDifferenceReason ( model , threshold = 100 )) indices = doubt . get_indices ( X , y ) from_predict ( pred , y , threshold ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 @staticmethod def from_predict ( pred , y , threshold ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import AbsoluteDifferenceReason y = np.random.randn(100) preds = np.random.randn(100) predicate = AbsoluteDifferenceReason.from_predict(preds, y, threshold=0.1) ``` \"\"\" difference = np . abs ( pred - y ) return ( difference >= threshold ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import AbsoluteDifferenceReason y = np . random . randn ( 100 ) preds = np . random . randn ( 100 ) predicate = AbsoluteDifferenceReason . from_predict ( preds , y , threshold = 0.1 ) CleanlabReason \u00b6 Assign doubt when using the cleanlab heuristic. Parameters Name Type Description Default model scikit-learn outlier model required sorted_index_method method used by cleanlab for sorting indices 'normalized_margin' min_doubt the minimum doubt output value used for sorting by the ensemble 0.5 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import CleanlabReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = CleanlabReason ( model )) indices = doubt . get_indices ( X , y ) from_proba ( proba , y , min_doubt = 0.5 , sorted_index_method = 'normalized_margin' ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 @staticmethod def from_proba ( proba , y , min_doubt = 0.5 , sorted_index_method = \"normalized_margin\" ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import CleanlabReason probas = np.array([[0.9, 0.1], [0.5, 0.5]]) y = np.array([0, 1]) predicate = CleanlabReason.from_proba(probas, y) ``` \"\"\" ordered_label_errors = get_noise_indices ( y , proba , sorted_index_method ) result = np . zeros_like ( y ) conf_arr = np . linspace ( 1 , min_doubt , result . shape [ 0 ]) for idx , _ in zip ( ordered_label_errors , conf_arr ): result [ idx ] = 1 return result . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import CleanlabReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ]]) y = np . array ([ 0 , 1 ]) predicate = CleanlabReason . from_proba ( probas , y ) DisagreeReason \u00b6 Assign doubt when two scikit-learn models disagree on a prediction. Parameters Name Type Description Default model1 scikit-learn classifier required model2 a different scikit-learn classifier required Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import DisagreeReason X , y = load_iris ( return_X_y = True ) model1 = LogisticRegression ( max_iter = 1_000 ) model2 = KNeighborsClassifier () model1 . fit ( X , y ) model2 . fit ( X , y ) doubt = DoubtEnsemble ( reason = DisagreeReason ( model1 , model2 )) indices = doubt . get_indices ( X , y ) from_pred ( pred1 , pred2 ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 @staticmethod def from_pred ( pred1 , pred2 ): \"\"\" Outputs a reason array from two pred arrays, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import DisagreeReason pred1 = [0, 1, 2] pred2 = [0, 1, 1] predicate = DisagreeReason.from_pred(pred1, pred2) assert np.all(predicate == np.array([0.0, 0.0, 1.0])) ``` \"\"\" return ( np . array ( pred1 ) != np . array ( pred2 )) . astype ( np . float16 ) Outputs a reason array from two pred arrays, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import DisagreeReason pred1 = [ 0 , 1 , 2 ] pred2 = [ 0 , 1 , 1 ] predicate = DisagreeReason . from_pred ( pred1 , pred2 ) assert np . all ( predicate == np . array ([ 0.0 , 0.0 , 1.0 ])) LongConfidenceReason \u00b6 Assign doubt when a wrong class gains too much confidence. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import LongConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = LongConfidenceReason ( model = model )) indices = doubt . get_indices ( X , y ) from_proba ( proba , y , classes , threshold ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 @staticmethod def from_proba ( proba , y , classes , threshold ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import LongConfidenceReason probas = np.array([[0.9, 0.1], [0.5, 0.5], [0.2, 0.8]]) y = np.array([0, 1, 0]) classes = np.array([0, 1]) threshold = 0.4 predicate = LongConfidenceReason.from_proba(probas, y, classes, threshold) assert np.all(predicate == np.array([0.0, 1.0, 1.0])) ``` \"\"\" mapper = { k : i for i , k in enumerate ( classes )} y_int = np . array ([ mapper [ k ] for k in y ]) confidences = proba . copy () # Advanced indexing trick: # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing confidences [ np . arange ( proba . shape [ 0 ]), y_int ] = 0 return ( confidences . max ( axis = 1 ) > threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import LongConfidenceReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ], [ 0.2 , 0.8 ]]) y = np . array ([ 0 , 1 , 0 ]) classes = np . array ([ 0 , 1 ]) threshold = 0.4 predicate = LongConfidenceReason . from_proba ( probas , y , classes , threshold ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 , 1.0 ])) MarginConfidenceReason \u00b6 Assign doubt when the difference between the top two most confident classes is too small. Throws an error when there are only two classes. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import MarginConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = MarginConfidenceReason ( model = model )) indices = doubt . get_indices ( X , y ) from_proba ( proba , threshold = 0.2 ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 @staticmethod def from_proba ( proba , threshold = 0.2 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import MarginConfidenceReason probas = np.array([[0.9, 0.1, 0.0], [0.5, 0.4, 0.1]]) predicate = MarginConfidenceReason.from_proba(probas, threshold=0.3) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" sorted = np . sort ( proba , axis = 1 ) margin = sorted [:, - 1 ] - sorted [:, - 2 ] return ( margin < threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import MarginConfidenceReason probas = np . array ([[ 0.9 , 0.1 , 0.0 ], [ 0.5 , 0.4 , 0.1 ]]) predicate = MarginConfidenceReason . from_proba ( probas , threshold = 0.3 ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ])) OutlierReason \u00b6 Assign doubt when a scikit-learn outlier model detects an outlier. Parameters Name Type Description Default model scikit-learn outlier model required Usage: from sklearn.datasets import load_iris from sklearn.ensemble import IsolationForest from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import OutlierReason X , y = load_iris ( return_X_y = True ) model = IsolationForest () model . fit ( X ) doubt = DoubtEnsemble ( reason = OutlierReason ( model )) indices = doubt . get_indices ( X , y ) ProbaReason \u00b6 Assign doubt based on low proba-confidence values from a scikit-learn model. Parameters Name Type Description Default model scikit-learn classifier required max_proba maximum probability threshold for doubt assignment 0.55 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ProbaReason ( model , max_proba = 0.55 )) indices = doubt . get_indices ( X , y ) from_proba ( proba , max_proba = 0.55 ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @staticmethod def from_proba ( proba , max_proba = 0.55 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ProbaReason probas = np.array([[0.9, 0.1], [0.5, 0.5]]) predicate = ProbaReason.from_proba(probas) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" return ( proba . max ( axis = 1 ) <= max_proba ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ProbaReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ]]) predicate = ProbaReason . from_proba ( probas ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ])) RandomReason \u00b6 Assign doubt based on a random value. Parameters Name Type Description Default probability probability of assigning a doubt 0.01 random_seed seed for random number generator 42 Usage: from sklearn.datasets import load_iris from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import RandomReason X , y = load_iris ( return_X_y = True ) doubt = DoubtEnsemble ( reason = RandomReason ( probability = 0.05 , random_seed = 42 )) indices = doubt . get_indices ( X , y ) RelativeDifferenceReason \u00b6 Assign doubt when the relative difference between label and regression is too large. Parameters Name Type Description Default model scikit-learn regression model required threshold cutoff for doubt assignment required Usage: from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import RelativeDifferenceReason X , y = load_diabetes ( return_X_y = True ) model = LinearRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = RelativeDifferenceReason ( model , threshold = 0.5 )) indices = doubt . get_indices ( X , y ) from_predict ( pred , y , threshold ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 @staticmethod def from_predict ( pred , y , threshold ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import RelativeDifferenceReason y = np.random.randn(100) preds = np.random.randn(100) predicate = RelativeDifferenceReason.from_predict(preds, y, threshold=0.1) ``` \"\"\" if np . any ( y == 0.0 ): raise ValueError ( \"Your `y` values contain 0. Will cause divided by zero error.\" ) difference = np . abs ( pred - y ) / y return ( difference >= threshold ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import RelativeDifferenceReason y = np . random . randn ( 100 ) preds = np . random . randn ( 100 ) predicate = RelativeDifferenceReason . from_predict ( preds , y , threshold = 0.1 ) ShannonEntropyReason \u00b6 Assign doubt when the normalized Shannon entropy is too high, see here for a discussion. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.5 smoothing constant value added to probas to prevent division by zeor 1e-05 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ShannonEntropyReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ShannonEntropyReason ( model = model )) indices = doubt . get_indices ( X , y ) from_proba ( proba , threshold = 0.5 , smoothing = 1e-05 ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @staticmethod def from_proba ( proba , threshold = 0.5 , smoothing = 1e-5 ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ShannonEntropyReason probas = np.array([[0.9, 0.1, 0.0], [0.5, 0.4, 0.1]]) predicate = ShannonEntropyReason.from_proba(probas, threshold=0.8) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" probas = proba + smoothing entropies = - ( probas * np . log ( probas ) / np . log ( probas . shape [ 1 ])) . sum ( axis = 1 ) return ( entropies > threshold ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ShannonEntropyReason probas = np . array ([[ 0.9 , 0.1 , 0.0 ], [ 0.5 , 0.4 , 0.1 ]]) predicate = ShannonEntropyReason . from_proba ( probas , threshold = 0.8 ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ])) ShortConfidenceReason \u00b6 Assign doubt when the correct class gains too little confidence. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ShortConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ShortConfidenceReason ( model = model , threshold = 0.4 )) indices = doubt . get_indices ( X , y ) from_proba ( proba , y , classes , threshold = 0.2 ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 @staticmethod def from_proba ( proba , y , classes , threshold = 0.2 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ShortConfidenceReason probas = np.array([[0.9, 0.1], [0.5, 0.5], [0.3, 0.7]]) y = np.array([0, 1, 0]) classes = np.array([0, 1]) threshold = 0.4 predicate = ShortConfidenceReason.from_proba(probas, y, classes, threshold) assert np.all(predicate == np.array([0.0, 0.0, 1.0])) ``` \"\"\" mapper = { k : i for i , k in enumerate ( classes )} y_int = np . array ([ mapper [ k ] for k in y ]) # Advanced indexing trick: # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing confidences = proba [ np . arange ( proba . shape [ 0 ]), y_int ] return ( confidences < threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ShortConfidenceReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ], [ 0.3 , 0.7 ]]) y = np . array ([ 0 , 1 , 0 ]) classes = np . array ([ 0 , 1 ]) threshold = 0.4 predicate = ShortConfidenceReason . from_proba ( probas , y , classes , threshold ) assert np . all ( predicate == np . array ([ 0.0 , 0.0 , 1.0 ])) StandardizedErrorReason \u00b6 Assign doubt when the absolute standardized residual is too high. Parameters Name Type Description Default model scikit-learn regression model required threshold cutoff for doubt assignment 2.0 Usage: from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import StandardizedErrorReason X , y = load_diabetes ( return_X_y = True ) model = LinearRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = StandardizedErrorReason ( model , threshold = 2. )) indices = doubt . get_indices ( X , y ) from_predict ( pred , y , threshold ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 @staticmethod def from_predict ( pred , y , threshold ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import StandardizedErrorReason y = np.random.randn(100) preds = np.random.randn(100) predicate = StandardizedErrorReason.from_predict(preds, y, threshold=0.1) ``` \"\"\" res = y - pred res_std = res / np . std ( res , ddof = 1 ) return ( np . abs ( res_std ) >= threshold ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import StandardizedErrorReason y = np . random . randn ( 100 ) preds = np . random . randn ( 100 ) predicate = StandardizedErrorReason . from_predict ( preds , y , threshold = 0.1 ) WrongPredictionReason \u00b6 Assign doubt when the model prediction doesn't match the label. Parameters Name Type Description Default model scikit-learn classifier required Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = WrongPredictionReason ( model = model )) indices = doubt . get_indices ( X , y ) from_predict ( pred , y ) (staticmethod) \u00b6 Show source code in doubtlab/reason.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @staticmethod def from_predict ( pred , y ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import WrongPredictionReason preds = np.array([\"positive\", \"negative\"]) y = np.array([\"positive\", \"neutral\"]) predicate = WrongPredictionReason.from_predict(preds, y) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" return ( pred != y ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import WrongPredictionReason preds = np . array ([ \"positive\" , \"negative\" ]) y = np . array ([ \"positive\" , \"neutral\" ]) predicate = WrongPredictionReason . from_predict ( preds , y ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ]))","title":"reasons"},{"location":"api/reasons/#from-doubtlabreason-import","text":"","title":"from doubtlab.reason import *"},{"location":"api/reasons/#doubtlab.reason.AbsoluteDifferenceReason","text":"Assign doubt when the absolute difference between label and regression is too large. Parameters Name Type Description Default model scikit-learn regression model required threshold cutoff for doubt assignment required Usage: from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import AbsoluteDifferenceReason X , y = load_diabetes ( return_X_y = True ) model = LinearRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = AbsoluteDifferenceReason ( model , threshold = 100 )) indices = doubt . get_indices ( X , y )","title":"AbsoluteDifferenceReason"},{"location":"api/reasons/#doubtlab.reason.AbsoluteDifferenceReason.from_predict","text":"Show source code in doubtlab/reason.py 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 @staticmethod def from_predict ( pred , y , threshold ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import AbsoluteDifferenceReason y = np.random.randn(100) preds = np.random.randn(100) predicate = AbsoluteDifferenceReason.from_predict(preds, y, threshold=0.1) ``` \"\"\" difference = np . abs ( pred - y ) return ( difference >= threshold ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import AbsoluteDifferenceReason y = np . random . randn ( 100 ) preds = np . random . randn ( 100 ) predicate = AbsoluteDifferenceReason . from_predict ( preds , y , threshold = 0.1 )","title":"from_predict()"},{"location":"api/reasons/#doubtlab.reason.CleanlabReason","text":"Assign doubt when using the cleanlab heuristic. Parameters Name Type Description Default model scikit-learn outlier model required sorted_index_method method used by cleanlab for sorting indices 'normalized_margin' min_doubt the minimum doubt output value used for sorting by the ensemble 0.5 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import CleanlabReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = CleanlabReason ( model )) indices = doubt . get_indices ( X , y )","title":"CleanlabReason"},{"location":"api/reasons/#doubtlab.reason.CleanlabReason.from_proba","text":"Show source code in doubtlab/reason.py 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 @staticmethod def from_proba ( proba , y , min_doubt = 0.5 , sorted_index_method = \"normalized_margin\" ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import CleanlabReason probas = np.array([[0.9, 0.1], [0.5, 0.5]]) y = np.array([0, 1]) predicate = CleanlabReason.from_proba(probas, y) ``` \"\"\" ordered_label_errors = get_noise_indices ( y , proba , sorted_index_method ) result = np . zeros_like ( y ) conf_arr = np . linspace ( 1 , min_doubt , result . shape [ 0 ]) for idx , _ in zip ( ordered_label_errors , conf_arr ): result [ idx ] = 1 return result . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import CleanlabReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ]]) y = np . array ([ 0 , 1 ]) predicate = CleanlabReason . from_proba ( probas , y )","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.DisagreeReason","text":"Assign doubt when two scikit-learn models disagree on a prediction. Parameters Name Type Description Default model1 scikit-learn classifier required model2 a different scikit-learn classifier required Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import DisagreeReason X , y = load_iris ( return_X_y = True ) model1 = LogisticRegression ( max_iter = 1_000 ) model2 = KNeighborsClassifier () model1 . fit ( X , y ) model2 . fit ( X , y ) doubt = DoubtEnsemble ( reason = DisagreeReason ( model1 , model2 )) indices = doubt . get_indices ( X , y )","title":"DisagreeReason"},{"location":"api/reasons/#doubtlab.reason.DisagreeReason.from_pred","text":"Show source code in doubtlab/reason.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 @staticmethod def from_pred ( pred1 , pred2 ): \"\"\" Outputs a reason array from two pred arrays, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import DisagreeReason pred1 = [0, 1, 2] pred2 = [0, 1, 1] predicate = DisagreeReason.from_pred(pred1, pred2) assert np.all(predicate == np.array([0.0, 0.0, 1.0])) ``` \"\"\" return ( np . array ( pred1 ) != np . array ( pred2 )) . astype ( np . float16 ) Outputs a reason array from two pred arrays, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import DisagreeReason pred1 = [ 0 , 1 , 2 ] pred2 = [ 0 , 1 , 1 ] predicate = DisagreeReason . from_pred ( pred1 , pred2 ) assert np . all ( predicate == np . array ([ 0.0 , 0.0 , 1.0 ]))","title":"from_pred()"},{"location":"api/reasons/#doubtlab.reason.LongConfidenceReason","text":"Assign doubt when a wrong class gains too much confidence. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import LongConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = LongConfidenceReason ( model = model )) indices = doubt . get_indices ( X , y )","title":"LongConfidenceReason"},{"location":"api/reasons/#doubtlab.reason.LongConfidenceReason.from_proba","text":"Show source code in doubtlab/reason.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 @staticmethod def from_proba ( proba , y , classes , threshold ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import LongConfidenceReason probas = np.array([[0.9, 0.1], [0.5, 0.5], [0.2, 0.8]]) y = np.array([0, 1, 0]) classes = np.array([0, 1]) threshold = 0.4 predicate = LongConfidenceReason.from_proba(probas, y, classes, threshold) assert np.all(predicate == np.array([0.0, 1.0, 1.0])) ``` \"\"\" mapper = { k : i for i , k in enumerate ( classes )} y_int = np . array ([ mapper [ k ] for k in y ]) confidences = proba . copy () # Advanced indexing trick: # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing confidences [ np . arange ( proba . shape [ 0 ]), y_int ] = 0 return ( confidences . max ( axis = 1 ) > threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import LongConfidenceReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ], [ 0.2 , 0.8 ]]) y = np . array ([ 0 , 1 , 0 ]) classes = np . array ([ 0 , 1 ]) threshold = 0.4 predicate = LongConfidenceReason . from_proba ( probas , y , classes , threshold ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 , 1.0 ]))","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.MarginConfidenceReason","text":"Assign doubt when the difference between the top two most confident classes is too small. Throws an error when there are only two classes. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import MarginConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = MarginConfidenceReason ( model = model )) indices = doubt . get_indices ( X , y )","title":"MarginConfidenceReason"},{"location":"api/reasons/#doubtlab.reason.MarginConfidenceReason.from_proba","text":"Show source code in doubtlab/reason.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 @staticmethod def from_proba ( proba , threshold = 0.2 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import MarginConfidenceReason probas = np.array([[0.9, 0.1, 0.0], [0.5, 0.4, 0.1]]) predicate = MarginConfidenceReason.from_proba(probas, threshold=0.3) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" sorted = np . sort ( proba , axis = 1 ) margin = sorted [:, - 1 ] - sorted [:, - 2 ] return ( margin < threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import MarginConfidenceReason probas = np . array ([[ 0.9 , 0.1 , 0.0 ], [ 0.5 , 0.4 , 0.1 ]]) predicate = MarginConfidenceReason . from_proba ( probas , threshold = 0.3 ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ]))","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.OutlierReason","text":"Assign doubt when a scikit-learn outlier model detects an outlier. Parameters Name Type Description Default model scikit-learn outlier model required Usage: from sklearn.datasets import load_iris from sklearn.ensemble import IsolationForest from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import OutlierReason X , y = load_iris ( return_X_y = True ) model = IsolationForest () model . fit ( X ) doubt = DoubtEnsemble ( reason = OutlierReason ( model )) indices = doubt . get_indices ( X , y )","title":"OutlierReason"},{"location":"api/reasons/#doubtlab.reason.ProbaReason","text":"Assign doubt based on low proba-confidence values from a scikit-learn model. Parameters Name Type Description Default model scikit-learn classifier required max_proba maximum probability threshold for doubt assignment 0.55 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ProbaReason ( model , max_proba = 0.55 )) indices = doubt . get_indices ( X , y )","title":"ProbaReason"},{"location":"api/reasons/#doubtlab.reason.ProbaReason.from_proba","text":"Show source code in doubtlab/reason.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @staticmethod def from_proba ( proba , max_proba = 0.55 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ProbaReason probas = np.array([[0.9, 0.1], [0.5, 0.5]]) predicate = ProbaReason.from_proba(probas) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" return ( proba . max ( axis = 1 ) <= max_proba ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ProbaReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ]]) predicate = ProbaReason . from_proba ( probas ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ]))","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.RandomReason","text":"Assign doubt based on a random value. Parameters Name Type Description Default probability probability of assigning a doubt 0.01 random_seed seed for random number generator 42 Usage: from sklearn.datasets import load_iris from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import RandomReason X , y = load_iris ( return_X_y = True ) doubt = DoubtEnsemble ( reason = RandomReason ( probability = 0.05 , random_seed = 42 )) indices = doubt . get_indices ( X , y )","title":"RandomReason"},{"location":"api/reasons/#doubtlab.reason.RelativeDifferenceReason","text":"Assign doubt when the relative difference between label and regression is too large. Parameters Name Type Description Default model scikit-learn regression model required threshold cutoff for doubt assignment required Usage: from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import RelativeDifferenceReason X , y = load_diabetes ( return_X_y = True ) model = LinearRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = RelativeDifferenceReason ( model , threshold = 0.5 )) indices = doubt . get_indices ( X , y )","title":"RelativeDifferenceReason"},{"location":"api/reasons/#doubtlab.reason.RelativeDifferenceReason.from_predict","text":"Show source code in doubtlab/reason.py 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 @staticmethod def from_predict ( pred , y , threshold ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import RelativeDifferenceReason y = np.random.randn(100) preds = np.random.randn(100) predicate = RelativeDifferenceReason.from_predict(preds, y, threshold=0.1) ``` \"\"\" if np . any ( y == 0.0 ): raise ValueError ( \"Your `y` values contain 0. Will cause divided by zero error.\" ) difference = np . abs ( pred - y ) / y return ( difference >= threshold ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import RelativeDifferenceReason y = np . random . randn ( 100 ) preds = np . random . randn ( 100 ) predicate = RelativeDifferenceReason . from_predict ( preds , y , threshold = 0.1 )","title":"from_predict()"},{"location":"api/reasons/#doubtlab.reason.ShannonEntropyReason","text":"Assign doubt when the normalized Shannon entropy is too high, see here for a discussion. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.5 smoothing constant value added to probas to prevent division by zeor 1e-05 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ShannonEntropyReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ShannonEntropyReason ( model = model )) indices = doubt . get_indices ( X , y )","title":"ShannonEntropyReason"},{"location":"api/reasons/#doubtlab.reason.ShannonEntropyReason.from_proba","text":"Show source code in doubtlab/reason.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @staticmethod def from_proba ( proba , threshold = 0.5 , smoothing = 1e-5 ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ShannonEntropyReason probas = np.array([[0.9, 0.1, 0.0], [0.5, 0.4, 0.1]]) predicate = ShannonEntropyReason.from_proba(probas, threshold=0.8) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" probas = proba + smoothing entropies = - ( probas * np . log ( probas ) / np . log ( probas . shape [ 1 ])) . sum ( axis = 1 ) return ( entropies > threshold ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ShannonEntropyReason probas = np . array ([[ 0.9 , 0.1 , 0.0 ], [ 0.5 , 0.4 , 0.1 ]]) predicate = ShannonEntropyReason . from_proba ( probas , threshold = 0.8 ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ]))","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.ShortConfidenceReason","text":"Assign doubt when the correct class gains too little confidence. Parameters Name Type Description Default model scikit-learn classifier required threshold confidence threshold for doubt assignment 0.2 Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ShortConfidenceReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = ShortConfidenceReason ( model = model , threshold = 0.4 )) indices = doubt . get_indices ( X , y )","title":"ShortConfidenceReason"},{"location":"api/reasons/#doubtlab.reason.ShortConfidenceReason.from_proba","text":"Show source code in doubtlab/reason.py 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 @staticmethod def from_proba ( proba , y , classes , threshold = 0.2 ): \"\"\" Outputs a reason array from a proba array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import ShortConfidenceReason probas = np.array([[0.9, 0.1], [0.5, 0.5], [0.3, 0.7]]) y = np.array([0, 1, 0]) classes = np.array([0, 1]) threshold = 0.4 predicate = ShortConfidenceReason.from_proba(probas, y, classes, threshold) assert np.all(predicate == np.array([0.0, 0.0, 1.0])) ``` \"\"\" mapper = { k : i for i , k in enumerate ( classes )} y_int = np . array ([ mapper [ k ] for k in y ]) # Advanced indexing trick: # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing confidences = proba [ np . arange ( proba . shape [ 0 ]), y_int ] return ( confidences < threshold ) . astype ( np . float16 ) Outputs a reason array from a proba array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import ShortConfidenceReason probas = np . array ([[ 0.9 , 0.1 ], [ 0.5 , 0.5 ], [ 0.3 , 0.7 ]]) y = np . array ([ 0 , 1 , 0 ]) classes = np . array ([ 0 , 1 ]) threshold = 0.4 predicate = ShortConfidenceReason . from_proba ( probas , y , classes , threshold ) assert np . all ( predicate == np . array ([ 0.0 , 0.0 , 1.0 ]))","title":"from_proba()"},{"location":"api/reasons/#doubtlab.reason.StandardizedErrorReason","text":"Assign doubt when the absolute standardized residual is too high. Parameters Name Type Description Default model scikit-learn regression model required threshold cutoff for doubt assignment 2.0 Usage: from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import StandardizedErrorReason X , y = load_diabetes ( return_X_y = True ) model = LinearRegression () model . fit ( X , y ) doubt = DoubtEnsemble ( reason = StandardizedErrorReason ( model , threshold = 2. )) indices = doubt . get_indices ( X , y )","title":"StandardizedErrorReason"},{"location":"api/reasons/#doubtlab.reason.StandardizedErrorReason.from_predict","text":"Show source code in doubtlab/reason.py 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 @staticmethod def from_predict ( pred , y , threshold ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import StandardizedErrorReason y = np.random.randn(100) preds = np.random.randn(100) predicate = StandardizedErrorReason.from_predict(preds, y, threshold=0.1) ``` \"\"\" res = y - pred res_std = res / np . std ( res , ddof = 1 ) return ( np . abs ( res_std ) >= threshold ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import StandardizedErrorReason y = np . random . randn ( 100 ) preds = np . random . randn ( 100 ) predicate = StandardizedErrorReason . from_predict ( preds , y , threshold = 0.1 )","title":"from_predict()"},{"location":"api/reasons/#doubtlab.reason.WrongPredictionReason","text":"Assign doubt when the model prediction doesn't match the label. Parameters Name Type Description Default model scikit-learn classifier required Usage: from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import WrongPredictionReason X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) doubt = DoubtEnsemble ( reason = WrongPredictionReason ( model = model )) indices = doubt . get_indices ( X , y )","title":"WrongPredictionReason"},{"location":"api/reasons/#doubtlab.reason.WrongPredictionReason.from_predict","text":"Show source code in doubtlab/reason.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 @staticmethod def from_predict ( pred , y ): \"\"\" Outputs a reason array from a prediction array, skipping the need for a model. Usage: ```python import numpy as np from doubtlab.reason import WrongPredictionReason preds = np.array([\"positive\", \"negative\"]) y = np.array([\"positive\", \"neutral\"]) predicate = WrongPredictionReason.from_predict(preds, y) assert np.all(predicate == np.array([0.0, 1.0])) ``` \"\"\" return ( pred != y ) . astype ( np . float16 ) Outputs a reason array from a prediction array, skipping the need for a model. Usage: import numpy as np from doubtlab.reason import WrongPredictionReason preds = np . array ([ \"positive\" , \"negative\" ]) y = np . array ([ \"positive\" , \"neutral\" ]) predicate = WrongPredictionReason . from_predict ( preds , y ) assert np . all ( predicate == np . array ([ 0.0 , 1.0 ]))","title":"from_predict()"},{"location":"examples/google-emotions/","text":"This example is based on this blogpost . It is also the example that motivated the creation of this project. Google Emotions \u00b6 We're going to check for bad labels in the Google Emotions dataset. This dataset contains text from Reddit (so expect profanity) with emotion tags attached. There are 28 different tags and a single text can belong to more than one emotion. We'll explore the \"excitement\" emotion here, but the exercise can be repeated for many other emotions too. The dataset comes with a paper that lists details . When you read the paper, you'll observe that a genuine effort was taken to make a high quality dataset. There are 82 raters involved n labelling this dataset. Each example should have been at least 3 people checking it. The paper mentions that all the folks who rated were from India but spoke English natively. An effort was made to remove subreddits that were not safe for work or that contained too much vulgar tokens (according to a predefined word-list). An effort was made to balance different subreddits such that larger subreddits wouldn\u2019t bias the dataset. An effort was made to remove subreddits that didn\u2019t offer a variety of emotions. An effort was made to mask names of people as well as references to religions. An effort was made to, in hindsight, confirm that there is sufficient interrated correlation. Given that this is a dataset from Google , and the fact that there's a paper about it ... how hard would it be to find bad labels? Data Loading \u00b6 Let's load in a portion of the dataset. import pandas as pd df = pd . read_csv ( \"https://github.com/koaning/optimal-on-paper/raw/main/data/goemotions_1.csv\" ) Let's sample a few random rows and zoom in on the excitement column. label_of_interest = 'excitement' ( df [[ 'text' , label_of_interest ]] . loc [ lambda d : d [ label_of_interest ] == 0 ] . sample ( 4 )) This is a sample. text excitement 27233 my favourite singer ([NAME]) helped write one of the songs so i love it 0 1385 No i didn\u2019t all i know is that i binged 3 seasoms of it. 0 17077 I liked [NAME]... 0 55699 A \"wise\" man once told me: > DO > YOUR > OWN >RESEARCH >! 0 Again, we should remind folks that this is reddit data. Beware vulgar language. Models \u00b6 Let's set up two modelling pipelines to detect the emotion. Let's start with a simple CountVectorizer model. from sklearn.pipeline import make_pipeline from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import CountVectorizer X , y = list ( df [ 'text' ]), df [ label_of_interest ] pipe = make_pipeline ( CountVectorizer (), LogisticRegression ( class_weight = 'balanced' , max_iter = 1000 ) ) Next, let's also make a pipeline that uses text embeddings. We'll use the whatlies library to do this. from sklearn.pipeline import make_union from whatlies.language import BytePairLanguage pipe_emb = make_pipeline ( make_union ( BytePairLanguage ( \"en\" , vs = 1_000 ), BytePairLanguage ( \"en\" , vs = 100_000 ) ), LogisticRegression ( class_weight = 'balanced' , max_iter = 1000 ) ) Let's train both pipelines before moving on. pipe . fit ( X , y ) pipe_emb . fit ( X , y ) Assign Doubt \u00b6 Let's now create a doubt ensemble using these two pipelines. from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , DisagreeReason , ShortConfidenceReason reasons = { 'proba' : ProbaReason ( pipe ), 'disagree' : DisagreeReason ( pipe , pipe_emb ), 'short_pipe' : ShortConfidenceReason ( pipe ), 'short_pipe_emb' : ShortConfidenceReason ( pipe_emb ), } doubt = DoubtEnsemble ( ** reasons ) There are four reasons in this ensemble. proba : This reason will assign doubt when the pipe pipeline doesn't predict any label with a high confidence. disagree : This reason will assign doubt when the pipe pipeline doesn't agree with the pipe_emb pipeline. short_pipe : This reason will assign doubt when the pipe pipeline predicts the correct label with a low confidence. short_pipe_emb : This reason will assign doubt when the pipe_emb pipeline predicts the correct label with a low confidence. All of these reasons have merit to it, but when they overlap we should assign extra attention. The DoubtEnsemble will assign the priority based on overlap on your behalf. Exploring Examples \u00b6 Let's explore some of the labels that deserve attention. # Return a dataframe with reasoning behind sorting predicates = doubt . get_predicates ( X , y ) # Use predicates to sort original dataframe df_sorted = df . iloc [ predicates . index ][[ 'text' , label_of_interest ]] # Create a dataframe containing predicates and original data df_label = pd . concat ([ df_sorted , predicates ], axis = 1 ) Let's check the first few rows of this dataframe. ( df_label [[ 'text' , label_of_interest ]] . head ( 10 )) text excitement Happy Easter everyone!! 0 Happy Easter everyone!! 0 Happy Easter everyone!! 0 Congratulations mate!! 0 Yes every time 0 New flavour! I love it! 0 Wow! Prayers for everyone there. 0 Wow! Prayers for everyone there. 0 Hey vro! 0 Oh my gooooooooood 0 There's some examples that certainly contain excitement. However, these are all examples where the label is 0. Let's re-use this dataframe one more time but now to explore examples where the data says there should be excitement. ( df_label [[ 'text' , label_of_interest ]] . loc [ lambda d : d [ 'excitement' ] == 1 ] . head ( 10 )) text excitement Hate Hate Hate, feels so good. 1 dear... husband 1 The old bear 1 I'd love to do that one day 1 [NAME] damn I love [NAME]. 1 [NAME] is a really cool name! 1 No haha but this is our first day on Reddit! 1 Yeah that pass 1 True! He probably is just lonely. Thank you for the kind words :) 1 a surprise to be sure 1 While some of the examples seem fine, I would argue that \"dear ... husband\" and \"The old bear\" are examples where the label is should be 0. Exploring Reasons \u00b6 It's worth doing a minor deep dive in the behavior behind the different reasons. None of the reasons are perfect, but they all favor different examples for reconsideration. CountVectorizer short on Confidence \u00b6 This is a \"high\"-bias bag-of-words model. It's going to likely overfit on the apperance of a token in the text. ( df_label . sort_values ( \"predicate_short_pipe\" , ascending = False ) . head ( 10 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement I am inexplicably excited by [NAME]. I get so excited by how he curls passes 0 Omg this is so amazing ! Keep up the awesome work and have a fantastic New Year ! 0 Sounds like a fun game. Our home game around here is .05/.10. Its fun but not very exciting. 0 So no replays for arsenal penalty calls.. Cool cool cool cool cool cool cool cool 0 Wow, your posting history is a real... interesting ride. 0 No different than people making a big deal about their team winning the super bowl. People find it interesting. 0 Hey congrats!! That's amazing, you've done such amazing progress! Hope you have a great day :) 0 I just read your list and now I can't wait, either!! Hurry up with the happy, relieved and peaceful onward and upward!! Congratulations\ud83d\ude0e 0 CountVectorizer with Low Proba \u00b6 This is a \"high\"-bias bag-of-words model when it isn't confident. It's going to likely overfit examples with tokens that appear in both classes. ( df_label . sort_values ( \"predicate_proba\" , ascending = False ) . head ( 10 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement Happy Easter everyone!! 0 This game is on [NAME]... 0 I swear if it's the Cowboys and the Patriots in the Super Bowl I'm going to burn something down. 0 I'm on red pills :) 0 Wow. I hope that asst manager will be looking for a new job soon. 0 No lie I was just fucking watching the office but I paused it and am know listening to graduation and browsing this subreddit 0 I was imagining her just coming in to work wearing the full [NAME] look. 0 Like this game from a week ago? 26 points 14 0 You should come. You'd enjoy it. 0 I almost pissed myself waiting so long in the tunnel. Not a fun feeling 0 BytePair Embeddings short on Confidence \u00b6 This is model based on just word embeddings. These embeddings are pooled together before being passed to the classifier which is likely why it favors short texts. ( df_label . sort_values ( \"predicate_short_pipe_emb\" , ascending = False ) . head ( 20 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement Woot woot! 0 WOW!!! 0 Happy birthday! 0 Happy Birthday! 0 Happy one week anniversary 0 Happy Birthday!!! 0 Pop pop! 0 Enjoy the ride! 0 Very interesting!!! 0 My exact reaction 0 happy birthday dude! 0 Enjoy 0 Oh wow!!! 0 This sounds interesting 0 Conclusion \u00b6 This example demonstrates two things. By combining reasons into an ensemble, we get a pretty good system to spot examples worth double checking. It's fairly easy to find bad labels, even in a dataset hosted by Google, even when there's an article written about it. This does not bode well for any models trained on this dataset. Required Nuance \u00b6 We think this example demonstrates the utility of doubtlab and that it also serves as a useful case-study that warns people of the dangers of label errors. That said, we want to mention a few points of nuance. The emotions dataset also comes with a column for the rater_id and example_very_unclear . Some of the examples that we've found using doubtlab also have disagreement between raters. The unclear-example flag is also raised a few times when we spot a bad label. One can only commend the authors for taking this effort because these columns help explain that some of the labels shouldn't be taken at face value. It also deserves mentioning that emotion detection is genuinely an incredibly hard task to label. There's so much context and culture involved in expressing emotion in a natural language that I cannot expect a \"pure label\" to even exist. Sarcasm detection is an unsolved problem. If sarcasm is unsolved, how on earth can we guarantee emotion detection or sentiment? Next Steps \u00b6 Feel free to repeat this exercise but with a different emotion or with different reasoning in the ensemble.","title":"Google Emotions"},{"location":"examples/google-emotions/#google-emotions","text":"We're going to check for bad labels in the Google Emotions dataset. This dataset contains text from Reddit (so expect profanity) with emotion tags attached. There are 28 different tags and a single text can belong to more than one emotion. We'll explore the \"excitement\" emotion here, but the exercise can be repeated for many other emotions too. The dataset comes with a paper that lists details . When you read the paper, you'll observe that a genuine effort was taken to make a high quality dataset. There are 82 raters involved n labelling this dataset. Each example should have been at least 3 people checking it. The paper mentions that all the folks who rated were from India but spoke English natively. An effort was made to remove subreddits that were not safe for work or that contained too much vulgar tokens (according to a predefined word-list). An effort was made to balance different subreddits such that larger subreddits wouldn\u2019t bias the dataset. An effort was made to remove subreddits that didn\u2019t offer a variety of emotions. An effort was made to mask names of people as well as references to religions. An effort was made to, in hindsight, confirm that there is sufficient interrated correlation. Given that this is a dataset from Google , and the fact that there's a paper about it ... how hard would it be to find bad labels?","title":"Google Emotions"},{"location":"examples/google-emotions/#data-loading","text":"Let's load in a portion of the dataset. import pandas as pd df = pd . read_csv ( \"https://github.com/koaning/optimal-on-paper/raw/main/data/goemotions_1.csv\" ) Let's sample a few random rows and zoom in on the excitement column. label_of_interest = 'excitement' ( df [[ 'text' , label_of_interest ]] . loc [ lambda d : d [ label_of_interest ] == 0 ] . sample ( 4 )) This is a sample. text excitement 27233 my favourite singer ([NAME]) helped write one of the songs so i love it 0 1385 No i didn\u2019t all i know is that i binged 3 seasoms of it. 0 17077 I liked [NAME]... 0 55699 A \"wise\" man once told me: > DO > YOUR > OWN >RESEARCH >! 0 Again, we should remind folks that this is reddit data. Beware vulgar language.","title":"Data Loading"},{"location":"examples/google-emotions/#models","text":"Let's set up two modelling pipelines to detect the emotion. Let's start with a simple CountVectorizer model. from sklearn.pipeline import make_pipeline from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import CountVectorizer X , y = list ( df [ 'text' ]), df [ label_of_interest ] pipe = make_pipeline ( CountVectorizer (), LogisticRegression ( class_weight = 'balanced' , max_iter = 1000 ) ) Next, let's also make a pipeline that uses text embeddings. We'll use the whatlies library to do this. from sklearn.pipeline import make_union from whatlies.language import BytePairLanguage pipe_emb = make_pipeline ( make_union ( BytePairLanguage ( \"en\" , vs = 1_000 ), BytePairLanguage ( \"en\" , vs = 100_000 ) ), LogisticRegression ( class_weight = 'balanced' , max_iter = 1000 ) ) Let's train both pipelines before moving on. pipe . fit ( X , y ) pipe_emb . fit ( X , y )","title":"Models"},{"location":"examples/google-emotions/#assign-doubt","text":"Let's now create a doubt ensemble using these two pipelines. from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , DisagreeReason , ShortConfidenceReason reasons = { 'proba' : ProbaReason ( pipe ), 'disagree' : DisagreeReason ( pipe , pipe_emb ), 'short_pipe' : ShortConfidenceReason ( pipe ), 'short_pipe_emb' : ShortConfidenceReason ( pipe_emb ), } doubt = DoubtEnsemble ( ** reasons ) There are four reasons in this ensemble. proba : This reason will assign doubt when the pipe pipeline doesn't predict any label with a high confidence. disagree : This reason will assign doubt when the pipe pipeline doesn't agree with the pipe_emb pipeline. short_pipe : This reason will assign doubt when the pipe pipeline predicts the correct label with a low confidence. short_pipe_emb : This reason will assign doubt when the pipe_emb pipeline predicts the correct label with a low confidence. All of these reasons have merit to it, but when they overlap we should assign extra attention. The DoubtEnsemble will assign the priority based on overlap on your behalf.","title":"Assign Doubt"},{"location":"examples/google-emotions/#exploring-examples","text":"Let's explore some of the labels that deserve attention. # Return a dataframe with reasoning behind sorting predicates = doubt . get_predicates ( X , y ) # Use predicates to sort original dataframe df_sorted = df . iloc [ predicates . index ][[ 'text' , label_of_interest ]] # Create a dataframe containing predicates and original data df_label = pd . concat ([ df_sorted , predicates ], axis = 1 ) Let's check the first few rows of this dataframe. ( df_label [[ 'text' , label_of_interest ]] . head ( 10 )) text excitement Happy Easter everyone!! 0 Happy Easter everyone!! 0 Happy Easter everyone!! 0 Congratulations mate!! 0 Yes every time 0 New flavour! I love it! 0 Wow! Prayers for everyone there. 0 Wow! Prayers for everyone there. 0 Hey vro! 0 Oh my gooooooooood 0 There's some examples that certainly contain excitement. However, these are all examples where the label is 0. Let's re-use this dataframe one more time but now to explore examples where the data says there should be excitement. ( df_label [[ 'text' , label_of_interest ]] . loc [ lambda d : d [ 'excitement' ] == 1 ] . head ( 10 )) text excitement Hate Hate Hate, feels so good. 1 dear... husband 1 The old bear 1 I'd love to do that one day 1 [NAME] damn I love [NAME]. 1 [NAME] is a really cool name! 1 No haha but this is our first day on Reddit! 1 Yeah that pass 1 True! He probably is just lonely. Thank you for the kind words :) 1 a surprise to be sure 1 While some of the examples seem fine, I would argue that \"dear ... husband\" and \"The old bear\" are examples where the label is should be 0.","title":"Exploring Examples"},{"location":"examples/google-emotions/#exploring-reasons","text":"It's worth doing a minor deep dive in the behavior behind the different reasons. None of the reasons are perfect, but they all favor different examples for reconsideration.","title":"Exploring Reasons"},{"location":"examples/google-emotions/#countvectorizer-short-on-confidence","text":"This is a \"high\"-bias bag-of-words model. It's going to likely overfit on the apperance of a token in the text. ( df_label . sort_values ( \"predicate_short_pipe\" , ascending = False ) . head ( 10 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement I am inexplicably excited by [NAME]. I get so excited by how he curls passes 0 Omg this is so amazing ! Keep up the awesome work and have a fantastic New Year ! 0 Sounds like a fun game. Our home game around here is .05/.10. Its fun but not very exciting. 0 So no replays for arsenal penalty calls.. Cool cool cool cool cool cool cool cool 0 Wow, your posting history is a real... interesting ride. 0 No different than people making a big deal about their team winning the super bowl. People find it interesting. 0 Hey congrats!! That's amazing, you've done such amazing progress! Hope you have a great day :) 0 I just read your list and now I can't wait, either!! Hurry up with the happy, relieved and peaceful onward and upward!! Congratulations\ud83d\ude0e 0","title":"CountVectorizer short on Confidence"},{"location":"examples/google-emotions/#countvectorizer-with-low-proba","text":"This is a \"high\"-bias bag-of-words model when it isn't confident. It's going to likely overfit examples with tokens that appear in both classes. ( df_label . sort_values ( \"predicate_proba\" , ascending = False ) . head ( 10 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement Happy Easter everyone!! 0 This game is on [NAME]... 0 I swear if it's the Cowboys and the Patriots in the Super Bowl I'm going to burn something down. 0 I'm on red pills :) 0 Wow. I hope that asst manager will be looking for a new job soon. 0 No lie I was just fucking watching the office but I paused it and am know listening to graduation and browsing this subreddit 0 I was imagining her just coming in to work wearing the full [NAME] look. 0 Like this game from a week ago? 26 points 14 0 You should come. You'd enjoy it. 0 I almost pissed myself waiting so long in the tunnel. Not a fun feeling 0","title":"CountVectorizer with Low Proba"},{"location":"examples/google-emotions/#bytepair-embeddings-short-on-confidence","text":"This is model based on just word embeddings. These embeddings are pooled together before being passed to the classifier which is likely why it favors short texts. ( df_label . sort_values ( \"predicate_short_pipe_emb\" , ascending = False ) . head ( 20 )[[ 'text' , label_of_interest ]] . drop_duplicates ()) text excitement Woot woot! 0 WOW!!! 0 Happy birthday! 0 Happy Birthday! 0 Happy one week anniversary 0 Happy Birthday!!! 0 Pop pop! 0 Enjoy the ride! 0 Very interesting!!! 0 My exact reaction 0 happy birthday dude! 0 Enjoy 0 Oh wow!!! 0 This sounds interesting 0","title":"BytePair Embeddings short on Confidence"},{"location":"examples/google-emotions/#conclusion","text":"This example demonstrates two things. By combining reasons into an ensemble, we get a pretty good system to spot examples worth double checking. It's fairly easy to find bad labels, even in a dataset hosted by Google, even when there's an article written about it. This does not bode well for any models trained on this dataset.","title":"Conclusion"},{"location":"examples/google-emotions/#required-nuance","text":"We think this example demonstrates the utility of doubtlab and that it also serves as a useful case-study that warns people of the dangers of label errors. That said, we want to mention a few points of nuance. The emotions dataset also comes with a column for the rater_id and example_very_unclear . Some of the examples that we've found using doubtlab also have disagreement between raters. The unclear-example flag is also raised a few times when we spot a bad label. One can only commend the authors for taking this effort because these columns help explain that some of the labels shouldn't be taken at face value. It also deserves mentioning that emotion detection is genuinely an incredibly hard task to label. There's so much context and culture involved in expressing emotion in a natural language that I cannot expect a \"pure label\" to even exist. Sarcasm detection is an unsolved problem. If sarcasm is unsolved, how on earth can we guarantee emotion detection or sentiment?","title":"Required Nuance"},{"location":"examples/google-emotions/#next-steps","text":"Feel free to repeat this exercise but with a different emotion or with different reasoning in the ensemble.","title":"Next Steps"},{"location":"quickstart/","text":"The goal of this document is to explain how the library works on a high-level. Datasets and Models \u00b6 You can use doubtlab to check your own datasets for bad labels. Many of the methods that we provide are based on the interaction between a dataset and a model trained on that dataset. For example; from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) This examples shows a logistic regression model trained on the load_iris dataset. The model is able to make predictions on the dataset and it's also able to output a confidence score via model.predict_proba(X) . You could wonder. What might it mean if the confidence values are low? What might it mean if our model cannot make an accurate prediction on a datapoint that it's trained on? In both of these cases, it could be that nothing is wrong. But you could argue that these datapoints may be worth double-checking. Pipeline of Doubt Reasons \u00b6 The doubtlab library allows you to define \"reasons\" to doubt the validity of a datapoint. The code below shows you how to build an ensemble of the two aforementioned reasons. from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason # Define the reasons with a name. reasons = { \"proba\" : ProbaReason ( model = model , max_proba = 0.55 ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } # Put all the reasons into an ensemble doubt = DoubtEnsemble ( ** reasons ) This ensemble represents a pipeline of reasons to doubt the validity of a label. Internal Details A DoubtEnsemble , technically, is just an ensemble of callables. You could also choose to use lambda functions to define a reason for doubt. The example below shows an example of a lambda function that's equivalent to what WrongPredictionReason would do. DoubtEnsemble ( wrong_pred = lambda X , y : ( model . predict ( X ) != y ) . astype ( float16 ) ) When it's time to infer doubt, the DoubtEnsemble will call each callable reason in order, passing X , y and listening for an array that contains \"doubt-scores\". These scores are just numbers, but they follow a few rules. When there is no doubt, the score should be zero The maximum doubt that a reason can emit is one The higher the doubt-score, the more likely doubt should be. For now the library emits 0/1 scores, but this may change in the future. Retreiving Examples to Check \u00b6 There are multiple ways of retreiving the examples to check from the doubt pipeline. Get Indices \u00b6 You could simply use the DoubtEnsemble.get_indices method to get the indices of the original data that are in doubt. # Get the ordered indices of examples worth checking again indices = doubt . get_indices ( X , y ) In this case, you'd get an array with 7 elements. array([ 77, 106, 126, 133, 83, 119, 70]) You can inspect the associated rows/labels of the examples via: X [ indices ], y [ indices ] Get Predicates \u00b6 While the indices are useful they don't tell you much about how the ordering took place. If you'd like to see more details, you can also retreive a dataframe with predicates that explain which rows triggered which reasons. # Get the predicates, or reasoning, behind the order predicates = doubt . get_predicates ( X , y ) The predicates dataframe contains a column for each reason. The index refers to the row number in the original dataset. Let's check the top 10 rows. predicates . head ( 10 ) predicate_proba predicate_wrong_pred 77 1 1 106 1 1 126 1 0 133 1 0 83 0 1 119 1 0 70 0 1 105 0 0 107 0 0 104 0 0 There's a few things to observe here. The ensemble assumes that overlap between reasons matter is a reason to give a row priority, moving it up in the dataframe. The .get_indices method tells you what deserves checking and only returns candidates worth checking. The .get_predicates method tries to explain why these rows deserve to be checked and therefore returns a dataframe with a row for each row in X . The index of the predicates dataframe refers to rows in our original X , y arrays. Why do this exercise? \u00b6 It's bad enough to have bad labels in your training data, but if you have bad labels in your validation then it's really game over for your machine learning models. There's ample evidence that many pre-trained academic models have suffered from this problem. So there's a legitimate concern that it may be a problem for your dataset as well. The hope is that this library makes it just a bit easier for folks do to check their datasets for bad labels. It's an exercise worth doing and the author of this library would love to hear anekdotes. Does this scale? \u00b6 You might be dealing with a large dataset, in which case you may want to be concious of compute time. Suppose you have a setup that looks something like: from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , ShortConfidenceReason , LongConfidenceReason # Suppose this dataset is very big and that this computation is heavy. X , y = load_big_dataset () model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) # This step might be expensive because internally we will be calling # `model.predict_proba(X)` a lot! ensemble = DoubtEnsemble ( proba = ProbaReason ( model ) short = ShortConfidenceReason ( model ), long = LongConfidenceReason ( model ) ) Then you might wonder if we're able to speed things up by precomputing our .predict_proba() -values. You could use lambda s, but you can also use common utility methods that have been added to the reason classes. Most of our reasons implement a from_pred or from_proba method that you can use. See the API for more details. That way, we can rewrite the code for a speedup. from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , ShortConfidenceReason , LongConfidenceReason # Suppose this dataset is very big and that this computation is heavy. X , y = load_big_dataset () model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) # Let's precalculate the proba values. probas = model . predict_proba ( X ) # We can re-use the probas below. Note that some reasons require extra information. ensemble = DoubtEnsemble ( proba = ProbaReason . from_proba ( probas ) short = ShortConfidenceReason . from_proba ( probas , y , classes = [ \"pos\" , \"neg\" ], threshold = 0.2 ), long = LongConfidenceReason . from_proba ( probas , y , classes = [ \"pos\" , \"neg\" ], threshold = 0.4 ) ) Next Steps \u00b6 You may get some more inspiration by checking some of the examples of this library. Once you're ready to give the library a spin we encourage you to explore the suite of reasons that this library supports. General Reasons \u00b6 RandomReason : assign doubt randomly, just for sure OutlierReason : assign doubt when the model declares a row an outlier Classification Reasons \u00b6 ProbaReason : assign doubt when a models' confidence-values are low for any label WrongPredictionReason : assign doubt when a model cannot predict the listed label ShortConfidenceReason : assign doubt when the correct label gains too little confidence LongConfidenceReason : assign doubt when a wrong label gains too much confidence MarginConfidenceReason : assign doubt when there's a small difference between the top two class confidences DisagreeReason : assign doubt when two models disagree on a prediction CleanlabReason : assign doubt according to cleanlab Regression Reasons \u00b6 AbsoluteDifferenceReason : assign doubt when the absolute difference is too high RelativeDifferenceReason : assign doubt when the relative difference is too high StandardizedErrorReason : assign doubt when the absolute standardized residual is too high If you think there's a reason missing, feel free to mention it on GitHub .","title":"Quickstart"},{"location":"quickstart/#datasets-and-models","text":"You can use doubtlab to check your own datasets for bad labels. Many of the methods that we provide are based on the interaction between a dataset and a model trained on that dataset. For example; from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression X , y = load_iris ( return_X_y = True ) model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) This examples shows a logistic regression model trained on the load_iris dataset. The model is able to make predictions on the dataset and it's also able to output a confidence score via model.predict_proba(X) . You could wonder. What might it mean if the confidence values are low? What might it mean if our model cannot make an accurate prediction on a datapoint that it's trained on? In both of these cases, it could be that nothing is wrong. But you could argue that these datapoints may be worth double-checking.","title":"Datasets and Models"},{"location":"quickstart/#pipeline-of-doubt-reasons","text":"The doubtlab library allows you to define \"reasons\" to doubt the validity of a datapoint. The code below shows you how to build an ensemble of the two aforementioned reasons. from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , WrongPredictionReason # Define the reasons with a name. reasons = { \"proba\" : ProbaReason ( model = model , max_proba = 0.55 ), \"wrong_pred\" : WrongPredictionReason ( model = model ), } # Put all the reasons into an ensemble doubt = DoubtEnsemble ( ** reasons ) This ensemble represents a pipeline of reasons to doubt the validity of a label. Internal Details A DoubtEnsemble , technically, is just an ensemble of callables. You could also choose to use lambda functions to define a reason for doubt. The example below shows an example of a lambda function that's equivalent to what WrongPredictionReason would do. DoubtEnsemble ( wrong_pred = lambda X , y : ( model . predict ( X ) != y ) . astype ( float16 ) ) When it's time to infer doubt, the DoubtEnsemble will call each callable reason in order, passing X , y and listening for an array that contains \"doubt-scores\". These scores are just numbers, but they follow a few rules. When there is no doubt, the score should be zero The maximum doubt that a reason can emit is one The higher the doubt-score, the more likely doubt should be. For now the library emits 0/1 scores, but this may change in the future.","title":"Pipeline of Doubt Reasons"},{"location":"quickstart/#retreiving-examples-to-check","text":"There are multiple ways of retreiving the examples to check from the doubt pipeline.","title":"Retreiving Examples to Check"},{"location":"quickstart/#get-indices","text":"You could simply use the DoubtEnsemble.get_indices method to get the indices of the original data that are in doubt. # Get the ordered indices of examples worth checking again indices = doubt . get_indices ( X , y ) In this case, you'd get an array with 7 elements. array([ 77, 106, 126, 133, 83, 119, 70]) You can inspect the associated rows/labels of the examples via: X [ indices ], y [ indices ]","title":"Get Indices"},{"location":"quickstart/#get-predicates","text":"While the indices are useful they don't tell you much about how the ordering took place. If you'd like to see more details, you can also retreive a dataframe with predicates that explain which rows triggered which reasons. # Get the predicates, or reasoning, behind the order predicates = doubt . get_predicates ( X , y ) The predicates dataframe contains a column for each reason. The index refers to the row number in the original dataset. Let's check the top 10 rows. predicates . head ( 10 ) predicate_proba predicate_wrong_pred 77 1 1 106 1 1 126 1 0 133 1 0 83 0 1 119 1 0 70 0 1 105 0 0 107 0 0 104 0 0 There's a few things to observe here. The ensemble assumes that overlap between reasons matter is a reason to give a row priority, moving it up in the dataframe. The .get_indices method tells you what deserves checking and only returns candidates worth checking. The .get_predicates method tries to explain why these rows deserve to be checked and therefore returns a dataframe with a row for each row in X . The index of the predicates dataframe refers to rows in our original X , y arrays.","title":"Get Predicates"},{"location":"quickstart/#why-do-this-exercise","text":"It's bad enough to have bad labels in your training data, but if you have bad labels in your validation then it's really game over for your machine learning models. There's ample evidence that many pre-trained academic models have suffered from this problem. So there's a legitimate concern that it may be a problem for your dataset as well. The hope is that this library makes it just a bit easier for folks do to check their datasets for bad labels. It's an exercise worth doing and the author of this library would love to hear anekdotes.","title":"Why do this exercise?"},{"location":"quickstart/#does-this-scale","text":"You might be dealing with a large dataset, in which case you may want to be concious of compute time. Suppose you have a setup that looks something like: from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , ShortConfidenceReason , LongConfidenceReason # Suppose this dataset is very big and that this computation is heavy. X , y = load_big_dataset () model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) # This step might be expensive because internally we will be calling # `model.predict_proba(X)` a lot! ensemble = DoubtEnsemble ( proba = ProbaReason ( model ) short = ShortConfidenceReason ( model ), long = LongConfidenceReason ( model ) ) Then you might wonder if we're able to speed things up by precomputing our .predict_proba() -values. You could use lambda s, but you can also use common utility methods that have been added to the reason classes. Most of our reasons implement a from_pred or from_proba method that you can use. See the API for more details. That way, we can rewrite the code for a speedup. from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , ShortConfidenceReason , LongConfidenceReason # Suppose this dataset is very big and that this computation is heavy. X , y = load_big_dataset () model = LogisticRegression ( max_iter = 1_000 ) model . fit ( X , y ) # Let's precalculate the proba values. probas = model . predict_proba ( X ) # We can re-use the probas below. Note that some reasons require extra information. ensemble = DoubtEnsemble ( proba = ProbaReason . from_proba ( probas ) short = ShortConfidenceReason . from_proba ( probas , y , classes = [ \"pos\" , \"neg\" ], threshold = 0.2 ), long = LongConfidenceReason . from_proba ( probas , y , classes = [ \"pos\" , \"neg\" ], threshold = 0.4 ) )","title":"Does this scale?"},{"location":"quickstart/#next-steps","text":"You may get some more inspiration by checking some of the examples of this library. Once you're ready to give the library a spin we encourage you to explore the suite of reasons that this library supports.","title":"Next Steps"},{"location":"quickstart/#general-reasons","text":"RandomReason : assign doubt randomly, just for sure OutlierReason : assign doubt when the model declares a row an outlier","title":"General Reasons"},{"location":"quickstart/#classification-reasons","text":"ProbaReason : assign doubt when a models' confidence-values are low for any label WrongPredictionReason : assign doubt when a model cannot predict the listed label ShortConfidenceReason : assign doubt when the correct label gains too little confidence LongConfidenceReason : assign doubt when a wrong label gains too much confidence MarginConfidenceReason : assign doubt when there's a small difference between the top two class confidences DisagreeReason : assign doubt when two models disagree on a prediction CleanlabReason : assign doubt according to cleanlab","title":"Classification Reasons"},{"location":"quickstart/#regression-reasons","text":"AbsoluteDifferenceReason : assign doubt when the absolute difference is too high RelativeDifferenceReason : assign doubt when the relative difference is too high StandardizedErrorReason : assign doubt when the absolute standardized residual is too high If you think there's a reason missing, feel free to mention it on GitHub .","title":"Regression Reasons"},{"location":"quickstart/benchmarks/","text":"The goal of this document is to explain how we might be able to measure the effectiveness of finding bad labels. Info This document shows a way to run benchmarks with this library. It deserves to be said that this part of the library is the most experimental and may change in the future as we receive community feedback. How to Measure \u00b6 There are a lot of reasons that you might doubt a sample of data. We prefer to limit ourselves to the generally effective methods out there. But how can we measure effectiveness? In reality we could only do that if we know which labels are bad and which are good. But if we knew that, we wouldn't need this library. Simulation \u00b6 That's why, as a proxy, we allow you to run benchmarks using simulations. While this certainly is not a perfect approach, it is also not wholly unreasonable. Here's how we add labels. Suppose that we have a dataset X with some labels y that we'd like to predict. If we assume that the labels y are correct we can simulate bad labels by designating a few labels to be shuffled. For the rows designated to be shuffled, we can now select the y values and change them. For classification problems we can flip the labels such that another label than the original y -label is chosen. For regression we can instead shuffle all the values. We can now pass this data to an ensemble and in hindsight see if we're able to uncover which values were flipped. At the very least, we should be able to confirm that if we sort based on our \"reasons\" that we select bad labels at a rate that's better than random. Demonstration \u00b6 Let's proceed by running a small demonstration. Dataset \u00b6 We'll use a subset of the clinc dataset for this demonstration. It's a dataset that contains text that might be used in a chatbot-like setting and the goal is to predict what the original intent behind the text might be. import numpy as np import pandas as pd url = \"https://raw.githubusercontent.com/koaning/optimal-on-paper/main/data/outofscope-intent-classification-dataset.csv\" df = pd . read_csv ( url ) . head ( 5000 ) df . sample ( 3 ) Here's what the sample of the data might look like: text label what is my visa credit limit credit_limit i want to eat something from turkey meal_suggestion what is life's meaning meaning_of_life The goal of this dataset is to classify the text into predefined categories. We're only looking at the top 5000 rows to keep the computation of this example lightweight. Let's start by formally making a X , y pair. X = list ( df [ 'text' ]) y = df [ 'label' ] Flipping Labels \u00b6 We can now use some utilities from the benchmarking submodule to flip the labels. from doubtlab.benchmark import flip_labels y_flip , flip_indicator = flip_labels ( y , n = 200 ) You'll now have; y_flip : which contains the original labels with 200 labels that are flipped. flip_indicator : which is a numpy array that indicates if a label did (with value 1.0) or did not (with value 0.0) got flipped. Info We're using flip_labels here because we're working on a classification task. If you were working on a regression task we recommend using shuffle_labels instead. from doubtlab.benchmark import shuffle_labels y_flip , flip_indicator = shuffle_labels ( y , n = 200 ) Ensemble \u00b6 Given that we now have data to compare against, let's make a DoubtEnsemble . from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline , make_union from sklearn.feature_extraction.text import CountVectorizer from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , ShortConfidenceReason , LongConfidenceReason model = make_pipeline ( CountVectorizer (), LogisticRegression ( max_iter = 1000 , class_weight = \"balanced\" ) ) model . fit ( X , y_flip ) ensemble = DoubtEnsemble ( proba = ProbaReason ( model ), short = ShortConfidenceReason ( model , threshold = 0.2 ), long = LongConfidenceReason ( model , threshold = 0.9 ), ) With an ensemble defined, we can now proceed by generating a dataframe with predicates. # First, get our dataframe with predicates predicate_df = ensemble . get_predicates ( X , y_flip ) Precision and Recall at k \u00b6 Given our sorted dataframe with predicates we can now wonder how well we did in finding bad labels. A common proxy for this is to consider the recall and precision \"at k\" metric. Precision at k : suppose that we look at the top 10 rows of data to check, how many of these turn out to be bad labels? What if we look at the top 20? That's what precision at k tells you. Recall at k : suppose that we look at the top 10 rows of data to check, what percentage of all the bad labels will we have found? What if we look at the top 20? That's what recall at k tells you. The idea here is that precision and recall are a bit at odds with eachother. The higher the value for k the larger your recall is bound to be but the harder it will be to guarantee a high precision. For lower k values you're only looking at the most likely candidates so you're more sure that you've got bad labels but you're also more sure that you don't have all of them. Plotting \u00b6 Let's now use the predicate dataframe with our idx_flip array from before to determine if our approach gives us a better than random statistics. Warning The plot you're about to see below is an altair chart . In an attempt to keep the library lightweight we're not including altair as a dependency. That means that you may need to install it first in order to get the charts to render. pip install altair If you're unfamiliar with altair, you may appreciate the calmcode course . from doubtlab.benchmark import plot_precision_recall_at_k # Let's plot some `precision/recall at k` statistics! plot_precision_recall_at_k ( predicate_df , flip_indicator , max_k = 2000 ) In this interactive chart (you can pan/zoom) you can see dashed lines for precision/recall if we were going to be pulling bad label candidates at random. The straight lines represent the values the precision/recall values that we would achieve with our ensemble. As you can see we do seem to achieve an uplift over picking items randomly. That said, we shouldn't pretend that this library guarantees that you'll only get bad examples! The highest precision we ever measure is around 12%, which means that you may only get an example that reserves relabelling every 10 items or so. What Causes Uplift? \u00b6 You might ask the question; what can we do to cause the biggest uplift here? It depends a bit on what you want, but we can cause a shift in behavior by using a different ensemble. Let's create one that only uses the ProbaReason . ensemble = DoubtEnsemble ( proba = ProbaReason ( model ), # short = ShortConfidenceReason(model, threshold=0.2), # long = LongConfidenceReason(model, threshold=0.9), ) If we re-run everything, here's what the new chart looks like. The recall performance is much worse, but notice how the first few examples have a very high precision! That said, we should remember that we're basing everything on a simulation here, so feel free to take everything with a grain of salt. Understanding \u00b6 The effectiveness of a DoubtEnsemble is something we'd like to study further. After all, there's a lot of variables to consider! A classification task with few classes might need to be treated differently than one with many classes. It's possible that text classification tasks might benefit from having multiple models, some of which are based on embeddings. Given another process of flipping labels we may also benefit from other components. We may want to consider different ways of sorting/weighting the different reasons in our ensemble. Maybe our reasons should emit a confidence value instead of just a 0/1 indicator. That way we may be able to use a confidence-like proxy. The current simulation may not reflect real life. Maybe there are better ways of simulating bad labels that we can consider for our benchmarks here. It's still a bit of an open problem. If you ever manage to run an interesting benchmark with some learnings, please let us know via an issue on GitHub .","title":"Benchmarks"},{"location":"quickstart/benchmarks/#how-to-measure","text":"There are a lot of reasons that you might doubt a sample of data. We prefer to limit ourselves to the generally effective methods out there. But how can we measure effectiveness? In reality we could only do that if we know which labels are bad and which are good. But if we knew that, we wouldn't need this library.","title":"How to Measure"},{"location":"quickstart/benchmarks/#simulation","text":"That's why, as a proxy, we allow you to run benchmarks using simulations. While this certainly is not a perfect approach, it is also not wholly unreasonable. Here's how we add labels. Suppose that we have a dataset X with some labels y that we'd like to predict. If we assume that the labels y are correct we can simulate bad labels by designating a few labels to be shuffled. For the rows designated to be shuffled, we can now select the y values and change them. For classification problems we can flip the labels such that another label than the original y -label is chosen. For regression we can instead shuffle all the values. We can now pass this data to an ensemble and in hindsight see if we're able to uncover which values were flipped. At the very least, we should be able to confirm that if we sort based on our \"reasons\" that we select bad labels at a rate that's better than random.","title":"Simulation"},{"location":"quickstart/benchmarks/#demonstration","text":"Let's proceed by running a small demonstration.","title":"Demonstration"},{"location":"quickstart/benchmarks/#dataset","text":"We'll use a subset of the clinc dataset for this demonstration. It's a dataset that contains text that might be used in a chatbot-like setting and the goal is to predict what the original intent behind the text might be. import numpy as np import pandas as pd url = \"https://raw.githubusercontent.com/koaning/optimal-on-paper/main/data/outofscope-intent-classification-dataset.csv\" df = pd . read_csv ( url ) . head ( 5000 ) df . sample ( 3 ) Here's what the sample of the data might look like: text label what is my visa credit limit credit_limit i want to eat something from turkey meal_suggestion what is life's meaning meaning_of_life The goal of this dataset is to classify the text into predefined categories. We're only looking at the top 5000 rows to keep the computation of this example lightweight. Let's start by formally making a X , y pair. X = list ( df [ 'text' ]) y = df [ 'label' ]","title":"Dataset"},{"location":"quickstart/benchmarks/#flipping-labels","text":"We can now use some utilities from the benchmarking submodule to flip the labels. from doubtlab.benchmark import flip_labels y_flip , flip_indicator = flip_labels ( y , n = 200 ) You'll now have; y_flip : which contains the original labels with 200 labels that are flipped. flip_indicator : which is a numpy array that indicates if a label did (with value 1.0) or did not (with value 0.0) got flipped. Info We're using flip_labels here because we're working on a classification task. If you were working on a regression task we recommend using shuffle_labels instead. from doubtlab.benchmark import shuffle_labels y_flip , flip_indicator = shuffle_labels ( y , n = 200 )","title":"Flipping Labels"},{"location":"quickstart/benchmarks/#ensemble","text":"Given that we now have data to compare against, let's make a DoubtEnsemble . from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline , make_union from sklearn.feature_extraction.text import CountVectorizer from doubtlab.ensemble import DoubtEnsemble from doubtlab.reason import ProbaReason , ShortConfidenceReason , LongConfidenceReason model = make_pipeline ( CountVectorizer (), LogisticRegression ( max_iter = 1000 , class_weight = \"balanced\" ) ) model . fit ( X , y_flip ) ensemble = DoubtEnsemble ( proba = ProbaReason ( model ), short = ShortConfidenceReason ( model , threshold = 0.2 ), long = LongConfidenceReason ( model , threshold = 0.9 ), ) With an ensemble defined, we can now proceed by generating a dataframe with predicates. # First, get our dataframe with predicates predicate_df = ensemble . get_predicates ( X , y_flip )","title":"Ensemble"},{"location":"quickstart/benchmarks/#precision-and-recall-at-k","text":"Given our sorted dataframe with predicates we can now wonder how well we did in finding bad labels. A common proxy for this is to consider the recall and precision \"at k\" metric. Precision at k : suppose that we look at the top 10 rows of data to check, how many of these turn out to be bad labels? What if we look at the top 20? That's what precision at k tells you. Recall at k : suppose that we look at the top 10 rows of data to check, what percentage of all the bad labels will we have found? What if we look at the top 20? That's what recall at k tells you. The idea here is that precision and recall are a bit at odds with eachother. The higher the value for k the larger your recall is bound to be but the harder it will be to guarantee a high precision. For lower k values you're only looking at the most likely candidates so you're more sure that you've got bad labels but you're also more sure that you don't have all of them.","title":"Precision and Recall at k"},{"location":"quickstart/benchmarks/#plotting","text":"Let's now use the predicate dataframe with our idx_flip array from before to determine if our approach gives us a better than random statistics. Warning The plot you're about to see below is an altair chart . In an attempt to keep the library lightweight we're not including altair as a dependency. That means that you may need to install it first in order to get the charts to render. pip install altair If you're unfamiliar with altair, you may appreciate the calmcode course . from doubtlab.benchmark import plot_precision_recall_at_k # Let's plot some `precision/recall at k` statistics! plot_precision_recall_at_k ( predicate_df , flip_indicator , max_k = 2000 ) In this interactive chart (you can pan/zoom) you can see dashed lines for precision/recall if we were going to be pulling bad label candidates at random. The straight lines represent the values the precision/recall values that we would achieve with our ensemble. As you can see we do seem to achieve an uplift over picking items randomly. That said, we shouldn't pretend that this library guarantees that you'll only get bad examples! The highest precision we ever measure is around 12%, which means that you may only get an example that reserves relabelling every 10 items or so.","title":"Plotting"},{"location":"quickstart/benchmarks/#what-causes-uplift","text":"You might ask the question; what can we do to cause the biggest uplift here? It depends a bit on what you want, but we can cause a shift in behavior by using a different ensemble. Let's create one that only uses the ProbaReason . ensemble = DoubtEnsemble ( proba = ProbaReason ( model ), # short = ShortConfidenceReason(model, threshold=0.2), # long = LongConfidenceReason(model, threshold=0.9), ) If we re-run everything, here's what the new chart looks like. The recall performance is much worse, but notice how the first few examples have a very high precision! That said, we should remember that we're basing everything on a simulation here, so feel free to take everything with a grain of salt.","title":"What Causes Uplift?"},{"location":"quickstart/benchmarks/#understanding","text":"The effectiveness of a DoubtEnsemble is something we'd like to study further. After all, there's a lot of variables to consider! A classification task with few classes might need to be treated differently than one with many classes. It's possible that text classification tasks might benefit from having multiple models, some of which are based on embeddings. Given another process of flipping labels we may also benefit from other components. We may want to consider different ways of sorting/weighting the different reasons in our ensemble. Maybe our reasons should emit a confidence value instead of just a 0/1 indicator. That way we may be able to use a confidence-like proxy. The current simulation may not reflect real life. Maybe there are better ways of simulating bad labels that we can consider for our benchmarks here. It's still a bit of an open problem. If you ever manage to run an interesting benchmark with some learnings, please let us know via an issue on GitHub .","title":"Understanding"},{"location":"quickstart/faq/","text":"How do I add a reason for nan values? \u00b6 A reason in doubtlab is little more than a function that can attach a 0/1 doubt-label to a row of data. As explained here , that means that you can totally use lambda functions! To implement this, you'll likely need to write something like: from doubtlab.ensemble import DoubtEnsemble ensemble = DoubtEnsemble ( wrong_pred = lambda X , y : ( model . predict ( X ) != y ) . astype ( float16 ), nan_label = lambda X , y : y . isnan (), ) Note that you can also add another reason for nan values that appear in X .","title":"FAQ"},{"location":"quickstart/faq/#how-do-i-add-a-reason-for-nan-values","text":"A reason in doubtlab is little more than a function that can attach a 0/1 doubt-label to a row of data. As explained here , that means that you can totally use lambda functions! To implement this, you'll likely need to write something like: from doubtlab.ensemble import DoubtEnsemble ensemble = DoubtEnsemble ( wrong_pred = lambda X , y : ( model . predict ( X ) != y ) . astype ( float16 ), nan_label = lambda X , y : y . isnan (), ) Note that you can also add another reason for nan values that appear in X .","title":"How do I add a reason for nan values?"}]}